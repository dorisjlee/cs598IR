{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "data = open(\"dblp-small.txt\",'r')\n",
    "dlines =data.readlines()\n",
    "#work with small subset for now\n",
    "# dlines = dlines[:100]\n",
    "all_words = []\n",
    "for l in dlines:\n",
    "    all_words.extend(l.split())\n",
    "vocab = unique(all_words)\n",
    "# Hyperparameters\n",
    "K = 20 # number of topics\n",
    "lmda = 0.9 #percentage of background "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10835\n",
      "1734168\n",
      "30140\n"
     ]
    }
   ],
   "source": [
    "print len(dlines)\n",
    "print len(all_words)\n",
    "print len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda$ specifies the percentage of the corpus that are generated from the background model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let $\\theta$ be a vector of topic distributions, where k=0 is the background model, and k+1...K is the real topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ndoc = len(dlines) #number of documents\n",
    "np.random.seed(999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(arr):\n",
    "    #Normalize document topic probabilities\n",
    "    normed_prob = []\n",
    "    if sum(arr)!=0: # prevent div/0 nans\n",
    "        for i in np.arange(len(arr)):\n",
    "            normed_prob.append(arr[i]/sum(arr[i]))\n",
    "        return np.array(normed_prob)\n",
    "    else: # if the whole array is zero\n",
    "        return np.array(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# normed_document_topic_prob = document_topic_prob.T/np.sum(document_topic_prob,axis=1) #normalize across documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step # 1\n",
      "Log likelihood: -247355.422185\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 2\n",
      "Log likelihood: -213208.470581\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 3\n",
      "Log likelihood: -184789.600094\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 4\n",
      "Log likelihood: -155554.738783\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 5\n",
      "Log likelihood: -130234.976775\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 6\n",
      "Log likelihood: -102738.835667\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 7\n",
      "Log likelihood: -83722.5617673\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 8\n",
      "Log likelihood: -66651.7071653\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 9\n",
      "Log likelihood: -50215.4235258\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 10\n",
      "Log likelihood: -37014.6456225\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 11\n",
      "Log likelihood: -20819.6551882\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 12\n",
      "Log likelihood: -6468.23937051\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 13\n",
      "Log likelihood: 2558.36519447\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 14\n",
      "Log likelihood: 10430.7060964\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 15\n",
      "Log likelihood: 23195.8143346\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 16\n",
      "Log likelihood: 27432.8042075\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 17\n",
      "Log likelihood: 36790.2675297\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 18\n",
      "Log likelihood: 42324.2907727\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 19\n",
      "Log likelihood: 51849.0353652\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 20\n",
      "Log likelihood: 54342.9414221\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 21\n",
      "Log likelihood: 57428.4579831\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 22\n",
      "Log likelihood: 62393.2275982\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 23\n",
      "Log likelihood: 66999.0609995\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 24\n",
      "Log likelihood: 71231.5342157\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 25\n",
      "Log likelihood: 72921.3179849\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 26\n",
      "Log likelihood: 74003.9872474\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 27\n",
      "Log likelihood: 72941.2888151\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 28\n",
      "Log likelihood: 75828.9682216\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 29\n",
      "Log likelihood: 79295.0894437\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 30\n",
      "Log likelihood: 82165.5519532\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 31\n",
      "Log likelihood: 81202.2059316\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 32\n",
      "Log likelihood: 81550.2568528\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 33\n",
      "Log likelihood: 82785.1424928\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 34\n",
      "Log likelihood: 84001.1289231\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 35\n",
      "Log likelihood: 77247.2183937\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 36\n",
      "Log likelihood: 77806.6557993\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 37\n",
      "Log likelihood: 76055.2600389\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 38\n",
      "Log likelihood: 82057.761158\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 39\n",
      "Log likelihood: 80839.7834482\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 40\n",
      "Log likelihood: 77421.8620217\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 41\n",
      "Log likelihood: 75104.1589661\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 42\n",
      "Log likelihood: 77557.2640843\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 43\n",
      "Log likelihood: 74569.4592339\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 44\n",
      "Log likelihood: 74227.5233516\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 45\n",
      "Log likelihood: 71945.9538712\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 46\n",
      "Log likelihood: 67640.386643\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 47\n",
      "Log likelihood: 65498.7879367\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 48\n",
      "Log likelihood: 63811.2705322\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 49\n",
      "Log likelihood: 64382.456048\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 50\n",
      "Log likelihood: 65630.9195182\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 51\n",
      "Log likelihood: 64487.332514\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 52\n",
      "Log likelihood: 63579.4358791\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 53\n",
      "Log likelihood: 61288.8854049\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 54\n",
      "Log likelihood: 62316.938102\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 55\n",
      "Log likelihood: 58991.1190621\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 56\n",
      "Log likelihood: 59891.8585345\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 57\n",
      "Log likelihood: 55620.1888767\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 58\n",
      "Log likelihood: 57007.963181\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 59\n",
      "Log likelihood: 56404.5021004\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 60\n",
      "Log likelihood: 54889.4338803\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 61\n",
      "Log likelihood: 51763.9140317\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 62\n",
      "Log likelihood: 52476.747952\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 63\n",
      "Log likelihood: 48091.8016065\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 64\n",
      "Log likelihood: 51193.550929\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 65\n",
      "Log likelihood: 44635.9299376\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 66\n",
      "Log likelihood: 43933.0815323\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 67\n",
      "Log likelihood: 43669.6525817\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 68\n",
      "Log likelihood: 41495.77175\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 69\n",
      "Log likelihood: 37455.9259291\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 70\n",
      "Log likelihood: 37362.3796131\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 71\n",
      "Log likelihood: 40002.2063869\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 72\n",
      "Log likelihood: 37465.7788758\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 73\n",
      "Log likelihood: 37058.7653318\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 74\n",
      "Log likelihood: 36171.8686881\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 75\n",
      "Log likelihood: 32832.5856359\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 76\n",
      "Log likelihood: 32433.567822\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 77\n",
      "Log likelihood: 30798.2894219\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 78\n",
      "Log likelihood: 29239.0246331\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 79\n",
      "Log likelihood: 31923.1182815\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 80\n",
      "Log likelihood: 30547.8545112\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 81\n",
      "Log likelihood: 26491.5667975\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 82\n",
      "Log likelihood: 29061.7157135\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 83\n",
      "Log likelihood: 26809.2546952\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 84\n",
      "Log likelihood: 27877.9413026\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 85\n",
      "Log likelihood: 23535.355185\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 86\n",
      "Log likelihood: 21839.8373785\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 87\n",
      "Log likelihood: 24620.2848021\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 88\n",
      "Log likelihood: 26090.6955427\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 89\n",
      "Log likelihood: 22890.9946623\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 90\n",
      "Log likelihood: 19400.6173074\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 91\n",
      "Log likelihood: 17992.6927069\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 92\n",
      "Log likelihood: 14079.4869198\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 93\n",
      "Log likelihood: 16530.5433382\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 94\n",
      "Log likelihood: 14884.4543162\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 95\n",
      "Log likelihood: 9596.24715193\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 96\n",
      "Log likelihood: 11648.2149988\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 97\n",
      "Log likelihood: 7559.91552223\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 98\n",
      "Log likelihood: 9638.04619877\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 99\n",
      "Log likelihood: 2954.91906402\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 100\n",
      "Log likelihood: 4987.5162503\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "#Random Initialization then normalize probabilities\n",
    "document_topic_prob = np.random.rand(ndoc, K+1 )  # P(z|pi) (K+1 topics for background)\n",
    "#Normalize document topic probabilities\n",
    "document_topic_prob = normalize(document_topic_prob)\n",
    "topic_word_prob = np.random.rand(K+1, len(vocab))# P(w | theta)\n",
    "topic_word_prob = normalize(topic_word_prob)\n",
    "#too large to do random.rand(ndoc, len(vocab), K+1) , so simply initialize with all zeroes \n",
    "topic_prob = np.random.rand(ndoc, len(vocab), K+1)# P(z | D,pi,theta)\n",
    "topic_prob = normalize(topic_prob)\n",
    "\n",
    "background_prob = np.random.rand(ndoc, len(vocab))# qy(y=1)=P(y=1| D,pi,theta)\n",
    "background_prob = normalize(background_prob)\n",
    "\n",
    "niter = 0\n",
    "prev_loglikelihood =0\n",
    "loglikelihood =999\n",
    "loglikelihood_lst = []\n",
    "while (abs(loglikelihood-prev_loglikelihood)>0.0001 or niter<=100): \n",
    "    print \"Step #\",niter+1\n",
    "    # Compute log p(D|theta, pi) at the ith step \n",
    "    loglikelihood = 0\n",
    "    for d in np.arange(len(dlines)):\n",
    "        for w in np.arange(len(vocab)):\n",
    "            loglikelihood+= log(lmda*background_prob[d,w]+(1-lmda)*sum(document_topic_prob[d,:]*topic_word_prob[:, w]))\n",
    "    print \"Log likelihood:\",str(loglikelihood)\n",
    "    if np.isnan(loglikelihood):\n",
    "        loglikelihood=99999\n",
    "    #matrix storing yij (if yij=0 then background; yij=1 then topic words)\n",
    "    Y = np.zeros([ndoc, len(vocab)])\n",
    "    print \"Expectation Step \"\n",
    "    qzy=0\n",
    "    qy=0\n",
    "    for d, document in enumerate(dlines):\n",
    "        for w in np.arange(len(vocab)):\n",
    "            if Y[d][w]==1: \n",
    "                # If Topic\n",
    "                qzy = document_topic_prob[d,:]*topic_word_prob[:, w]\n",
    "                #Finish computing qzy\n",
    "                qzy = normalize(prob)\n",
    "                topic_prob[d][w] = qzy\n",
    "            elif Y[d][w]==0: \n",
    "                # If Background, Computing q_y \n",
    "                numerator =0\n",
    "                for k in np.arange(K):\n",
    "                    numerator += sum(document_topic_prob[d,:]*topic_word_prob[:,w])\n",
    "                #Finish computing qy\n",
    "                numerator = (1-lmda)*numerator\n",
    "                p_wD = document.count(str(w))/len(document)\n",
    "                denominator = lmda*p_wD + numerator\n",
    "                qy = numerator/denominator\n",
    "                background_prob[d][w] = qy\n",
    "#             print \"qzy:\",qzy\n",
    "    print \"Maximization step  \"\n",
    "\n",
    "    # update p(z=k|pi) for the n+1th step\n",
    "    for z in np.arange(K):\n",
    "        for w in np.arange(len(vocab)):    \n",
    "            ndk=0\n",
    "            for d in np.arange(len(dlines)):\n",
    "                c = dlines[d].count(str(w)) #count of a word in the document \n",
    "                ndk += c*topic_prob[d,w,z]*background_prob[d,w]\n",
    "            print ndk\n",
    "            topic_word_prob[z][w]=ndk\n",
    "        #Normalize across all documents for each topic \n",
    "        topic_word_prob[z] = normalize(topic_word_prob[z]) \n",
    "    # update p(w|theta_k) for the n+1th step\n",
    "    for d in np.arange(len(dlines)):\n",
    "        for z in np.arange(K):\n",
    "            nwk=0\n",
    "            for w in np.arange(len(vocab)):    \n",
    "                c = dlines[d].count(str(w)) #count of a word in the document \n",
    "                nwk += c*topic_prob[d,w,z]*background_prob[d,w]\n",
    "            document_topic_prob[d][z]=nwk\n",
    "        #Normalize across all words in the document\n",
    "        document_topic_prob[d] = normalize(document_topic_prob[d]) \n",
    "    #Update iteration and log-likelihood\n",
    "    niter +=1\n",
    "    loglikelihood_lst.append(loglikelihood)\n",
    "    prev_loglikelihood = loglikelihood\n",
    "print \"Finished\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2)  The value of the log likelihood should be monotonically increasing as each iteration progresses, but the amount of increase in loglikelihood should slowly plateau as the iteration progresses. (This is the motivation for using the  dL<0.0001 stopping criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3)  Since the log likelihood at the i-1 th step should be larger than the one in the i-th step, the numerator is a positive quantity (assuming that the log-likelihood is positive). As the iterations progress, the difference between the log-likelihood of subsequent steps should get smaller. The denominator term should get larger (since log likelihood increases), so overall $\\Delta_i$ should decrease.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hw5_q3](hw5_q3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) These are the plots for the loglikelihood after running PLSA mixture model with K=20 and $\\lambda$=0.9. I was surprised to see that the loglikelihood decreased after around 30 iterations. My hypothesis was that the loglikelihood should not decrease because EM is a hill-climbing algorithm that has convergence guaruntee that each step has a higher likelihood than the previous step. The change in loglikelihood also did not match my heuristic expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1160b49d0>]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEACAYAAACQx1DIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1154c05d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loglikelihood_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delta = (np.roll(loglikelihood_lst,-1)-loglikelihood_lst)/np.roll(loglikelihood_lst,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x116246350>]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEACAYAAACnJV25AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116161950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For $\\lambda=0.9$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['precisionto' 'powersaving' 'shows' 'interferes' 'merely' 'acc' 'uportal'\n",
      " 'outagesare' 'comparable' 'possesses']\n",
      "['spht' 'tvd' 'rhombic' 'make' 'doorways' 'hour' 'umesh' 'topc' 'drying'\n",
      " 'nacks']\n",
      "['crl' 'textually' 'sentinel' 'protocolwe' 'deterrent' 'unremarkable' 'jsp'\n",
      " 'only' 'nobel' 'melodic']\n",
      "['tats' 'approches' 'definition' 'palettes' 'manufacturability'\n",
      " 'electromigration' 'abstractionsl' 'availablewe' 'make' 'iconic']\n",
      "['utterance' 'providesa' 'sparked' 'pressing' 'korea' 'wikis'\n",
      " 'attractivity' 'make' 'activation' 'clairvoyant']\n",
      "['broadcast' 'mtt' 'ispreadrank' 'and' 'amst' 'refereed'\n",
      " 'parallelizability' 'pmg' 'waga' 'secrets']\n",
      "['jffs' 'superlumt' 'ocaml' 'hampering' 'kyoto' 'competed' 'therewith'\n",
      " 'teenage' 'leverage' 'all']\n",
      "['reusability' 'grover' 'ours' 'predesigned' 'subdialogues' 'pentium'\n",
      " 'kazhdan' 'less' 'forlinks' 'apertures']\n",
      "['anl' 'uar' 'and' 'andmemory' 'pnl' 'jaffe' 'find' 'convolving'\n",
      " \"movement's\" 'schemeand']\n",
      "['knowledgebase' 'backtransactional' 'bets' 'and' 'salsa' 'ambienttalk'\n",
      " 'musicbrainz' 'privilege' \"verisoft's\" 'unigram']\n",
      "['verbal' 'translation' 'intersect' 'projected' 'harvesting' 'netscsi'\n",
      " 'karl' 'only' 'multilingual' 'prepcom']\n",
      "['octrees' 'gui' 'confidently' 'universities' 'consumptionwe'\n",
      " 'calculations' 'gskj' 'shows' 'insightful' 'underpinned']\n",
      "['kademlia' 'believable' 'collected' \"artist's\" 'mattssonhybrid' 'about'\n",
      " 'involving' 'borrows' 'poincare' 'usedin']\n",
      "['measuring' 'disposal' 'phaseload' 'about' 'constant' 'elgamal' 'pacc'\n",
      " 'hunting' 'downtown' 'pancyclic']\n",
      "['prehe' 'memorizable' 'greg' 'seek' 'csas' 'sign' 'frustration' 'less'\n",
      " 'derose' 'ose']\n",
      "['quel' 'fpl' 'wedges' 'pipeliner' 'varied' 'less' 'honoring' 'only'\n",
      " 'voluminos' 'sldnfa']\n",
      "['folds' 'all' 'perfcenter' 'donationbased' 'unranked' 'procedural' 'sp'\n",
      " 'matlab' 'techniqueused' 'reinforced']\n",
      "['rbfn' 'legislative' 'new' 'favorable' 'gerrymandering' 'jsp' 'aar'\n",
      " 'original' 'naming' 'cid']\n",
      "['overconstrained' 'user' 'fuzziness' 'beneficial' 'bgc' 'about'\n",
      " 'micropolygon' 'polytopes' 'solaria' 'brewka']\n",
      "['negated' 'loma' 'hardwure' 'republishing' 'marriage' 'patch' 'about'\n",
      " 'cause' 'ssuite' 'encumbers']\n",
      "['fared' 'the' 'channeled' 'hla' 'ptf' 'ineffectual' 'estimate' 'sebek'\n",
      " 'teradata' 'predeployed']\n"
     ]
    }
   ],
   "source": [
    "for topic in topic_word_prob:\n",
    "    print vocab[np.argsort(topic)[::-1][:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rerunning with $\\lambda =0.3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step # 1\n",
      "Log likelihood: -243483.892943\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 2\n",
      "Log likelihood: -212307.939543\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 3\n",
      "Log likelihood: -179714.459413\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 4\n",
      "Log likelihood: -153184.477998\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 5\n",
      "Log likelihood: -126049.900672\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 6\n",
      "Log likelihood: -98790.9318423\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 7\n",
      "Log likelihood: -81440.1344442\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 8\n",
      "Log likelihood: -62655.158832\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 9\n",
      "Log likelihood: -43998.0229784\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 10\n",
      "Log likelihood: -30664.6016262\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 11\n",
      "Log likelihood: -16226.0790391\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 12\n",
      "Log likelihood: -1834.16003328\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 13\n",
      "Log likelihood: 8380.71428287\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 14\n",
      "Log likelihood: 17692.3374474\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 15\n",
      "Log likelihood: 24410.2213005\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 16\n",
      "Log likelihood: 32126.575453\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 17\n",
      "Log likelihood: 38011.7461927\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 18\n",
      "Log likelihood: 48651.9786727\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 19\n",
      "Log likelihood: 52461.3780072\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 20\n",
      "Log likelihood: 53733.0102453\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 21\n",
      "Log likelihood: 57573.8806348\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 22\n",
      "Log likelihood: 64437.6414004\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 23\n",
      "Log likelihood: 66533.0471876\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 24\n",
      "Log likelihood: 69582.9785064\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 25\n",
      "Log likelihood: 70815.2199132\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 26\n",
      "Log likelihood: 73288.5868056\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 27\n",
      "Log likelihood: 80835.7373183\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 28\n",
      "Log likelihood: 83276.2913162\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 29\n",
      "Log likelihood: 79695.4933816\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 30\n",
      "Log likelihood: 80538.5172689\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 31\n",
      "Log likelihood: 81220.3613641\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 32\n",
      "Log likelihood: 85499.6746751\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 33\n",
      "Log likelihood: 82595.4618744\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 34\n",
      "Log likelihood: 84630.517017\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 35\n",
      "Log likelihood: 86721.6622948\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 36\n",
      "Log likelihood: 86485.6391816\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 37\n",
      "Log likelihood: 87735.238218\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 38\n",
      "Log likelihood: 82949.0686009\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 39\n",
      "Log likelihood: 83117.0977805\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 40\n",
      "Log likelihood: 78440.7217815\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 41\n",
      "Log likelihood: 82247.8618814\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 42\n",
      "Log likelihood: 81419.1067702\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 43\n",
      "Log likelihood: 80685.400573\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 44\n",
      "Log likelihood: 79056.7422977\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 45\n",
      "Log likelihood: 81825.168344\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 46\n",
      "Log likelihood: 78274.1488254\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 47\n",
      "Log likelihood: 80112.8442263\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 48\n",
      "Log likelihood: 81381.7179922\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 49\n",
      "Log likelihood: 78201.0230614\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 50\n",
      "Log likelihood: 78375.7264619\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 51\n",
      "Log likelihood: 77259.7792038\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 52\n",
      "Log likelihood: 75995.955661\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 53\n",
      "Log likelihood: 74110.9813045\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 54\n",
      "Log likelihood: 68476.9670844\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 55\n",
      "Log likelihood: 68643.6487238\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 56\n",
      "Log likelihood: 66675.092956\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 57\n",
      "Log likelihood: 65415.5989163\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 58\n",
      "Log likelihood: 61354.927082\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 59\n",
      "Log likelihood: 61560.7397159\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 60\n",
      "Log likelihood: 61755.0300163\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 61\n",
      "Log likelihood: 58513.4383921\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 62\n",
      "Log likelihood: 53417.4089155\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 63\n",
      "Log likelihood: 56434.2360717\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 64\n",
      "Log likelihood: 53017.9554155\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 65\n",
      "Log likelihood: 50870.6271917\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 66\n",
      "Log likelihood: 45429.5991954\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 67\n",
      "Log likelihood: 47191.669305\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 68\n",
      "Log likelihood: 45052.1727057\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 69\n",
      "Log likelihood: 46094.8456749\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 70\n",
      "Log likelihood: 42538.2720077\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 71\n",
      "Log likelihood: 41854.0334517\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 72\n",
      "Log likelihood: 41318.1625183\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 73\n",
      "Log likelihood: 41795.6803815\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 74\n",
      "Log likelihood: 42466.2728216\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 75\n",
      "Log likelihood: 40284.1365071\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 76\n",
      "Log likelihood: 38100.9391608\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 77\n",
      "Log likelihood: 35334.1232001\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 78\n",
      "Log likelihood: 36243.1045405\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 79\n",
      "Log likelihood: 35232.2134285\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 80\n",
      "Log likelihood: 34341.9760729\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 81\n",
      "Log likelihood: 31782.2297848\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 82\n",
      "Log likelihood: 29619.9765235\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 83\n",
      "Log likelihood: 26003.7834535\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 84\n",
      "Log likelihood: 26610.1567604\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 85\n",
      "Log likelihood: 24668.6455867\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 86\n",
      "Log likelihood: 23337.7131162\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 87\n",
      "Log likelihood: 27266.3893279\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 88\n",
      "Log likelihood: 22090.1945481\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 89\n",
      "Log likelihood: 19453.6808428\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 90\n",
      "Log likelihood: 19970.9923018\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 91\n",
      "Log likelihood: 22597.484323\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 92\n",
      "Log likelihood: 18474.4845007\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 93\n",
      "Log likelihood: 17844.9635002\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 94\n",
      "Log likelihood: 16834.2247342\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 95\n",
      "Log likelihood: 13472.2791518\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 96\n",
      "Log likelihood: 14471.064717\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 97\n",
      "Log likelihood: 10397.8300787\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 98\n",
      "Log likelihood: 10344.7955182\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 99\n",
      "Log likelihood: 3980.78159619\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Step # 100\n",
      "Log likelihood: 3497.40194362\n",
      "Expectation Step \n",
      "Maximization step  \n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "lmda=0.3\n",
    "K=20\n",
    "#Random Initialization then normalize probabilities\n",
    "document_topic_prob = np.random.rand(ndoc, K+1 )  # P(z|pi) (K+1 topics for background)\n",
    "#Normalize document topic probabilities\n",
    "document_topic_prob = normalize(document_topic_prob)\n",
    "topic_word_prob = np.random.rand(K+1, len(vocab))# P(w | theta)\n",
    "topic_word_prob = normalize(topic_word_prob)\n",
    "#too large to do random.rand(ndoc, len(vocab), K+1) , so simply initialize with all zeroes \n",
    "topic_prob = np.random.rand(ndoc, len(vocab), K+1)# P(z | D,pi,theta)\n",
    "topic_prob = normalize(topic_prob)\n",
    "\n",
    "background_prob = np.random.rand(ndoc, len(vocab))# qy(y=1)=P(y=1| D,pi,theta)\n",
    "background_prob = normalize(background_prob)\n",
    "\n",
    "niter = 0\n",
    "prev_loglikelihood =0\n",
    "loglikelihood =999\n",
    "loglikelihood_lst = []\n",
    "while (abs(loglikelihood-prev_loglikelihood)>0.0001 or niter<=100): \n",
    "    print \"Step #\",niter+1\n",
    "    # Compute log p(D|theta, pi) at the ith step \n",
    "    loglikelihood = 0\n",
    "    for d in np.arange(len(dlines)):\n",
    "        for w in np.arange(len(vocab)):\n",
    "            loglikelihood+= log(lmda*background_prob[d,w]+(1-lmda)*sum(document_topic_prob[d,:]*topic_word_prob[:, w]))\n",
    "    print \"Log likelihood:\",str(loglikelihood)\n",
    "    if np.isnan(loglikelihood):\n",
    "        loglikelihood=99999\n",
    "    #matrix storing yij (if yij=0 then background; yij=1 then topic words)\n",
    "    Y = np.zeros([ndoc, len(vocab)])\n",
    "    print \"Expectation Step \"\n",
    "    qzy=0\n",
    "    qy=0\n",
    "    for d, document in enumerate(dlines):\n",
    "        for w in np.arange(len(vocab)):\n",
    "            if Y[d][w]==1: \n",
    "                # If Topic\n",
    "                qzy = document_topic_prob[d,:]*topic_word_prob[:, w]\n",
    "                #Finish computing qzy\n",
    "                qzy = normalize(prob)\n",
    "                topic_prob[d][w] = qzy\n",
    "            elif Y[d][w]==0: \n",
    "                # If Background, Computing q_y \n",
    "                numerator =0\n",
    "                for k in np.arange(K):\n",
    "                    numerator += sum(document_topic_prob[d,:]*topic_word_prob[:,w])\n",
    "                #Finish computing qy\n",
    "                numerator = (1-lmda)*numerator\n",
    "                p_wD = document.count(str(w))/len(document)\n",
    "                denominator = lmda*p_wD + numerator\n",
    "                qy = numerator/denominator\n",
    "                background_prob[d][w] = qy\n",
    "#             print \"qzy:\",qzy\n",
    "    print \"Maximization step  \"\n",
    "\n",
    "    # update p(z=k|pi) for the n+1th step\n",
    "    for z in np.arange(K):\n",
    "        for w in np.arange(len(vocab)):    \n",
    "            ndk=0\n",
    "            for d in np.arange(len(dlines)):\n",
    "                c = dlines[d].count(str(w)) #count of a word in the document \n",
    "                ndk += c*topic_prob[d,w,z]*background_prob[d,w]\n",
    "            print ndk\n",
    "            topic_word_prob[z][w]=ndk\n",
    "        #Normalize across all documents for each topic \n",
    "        topic_word_prob[z] = normalize(topic_word_prob[z]) \n",
    "    # update p(w|theta_k) for the n+1th step\n",
    "    for d in np.arange(len(dlines)):\n",
    "        for z in np.arange(K):\n",
    "            nwk=0\n",
    "            for w in np.arange(len(vocab)):    \n",
    "                c = dlines[d].count(str(w)) #count of a word in the document \n",
    "                nwk += c*topic_prob[d,w,z]*background_prob[d,w]\n",
    "            document_topic_prob[d][z]=nwk\n",
    "        #Normalize across all words in the document\n",
    "        document_topic_prob[d] = normalize(document_topic_prob[d]) \n",
    "    #Update iteration and log-likelihood\n",
    "    niter +=1\n",
    "    loglikelihood_lst.append(loglikelihood)\n",
    "    prev_loglikelihood = loglikelihood\n",
    "print \"Finished\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We see that among the top words in the topics. There are more stopwords including 'all','with', 'by','the', as well as common words that don't convey too much meaning about a topic such as 'new','shows' and 'build'. This agrees with what we expect because as $\\lambda$ is set smaller, less words are identified as belonging to the background model. So this means that some of those background would get mixed into the real PLSA topics. As we see here, the PLSA topics are less clean (free of background words) when $\\lambda$ is set low but when $\\lambda$=0.9 then the PLSA are very discriminative and contains only a few background words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['precisionto' 'the' 'recoverywe' 'interferes' 'merely' 'acc' 'uportal'\n",
      " 'with' 'comparable' 'possesses']\n",
      "['spht' 'tvd' 'rhombic' 'statements' 'doorways' 'make' 'umesh' 'topc'\n",
      " 'drying' 'nacks']\n",
      "['crl' 'textually' 'sentinel' 'protocolwe' 'deterrent' 'unremarkable' 'jsp'\n",
      " \"store's\" 'nobel' 'melodic']\n",
      "['tats' 'approches' 'definition' 'palettes' 'manufacturability'\n",
      " 'electromigration' 'abstractionsl' 'availablewe' 'hyperplanes' 'build']\n",
      "['utterance' 'providesa' 'find' 'pressing' 'korea' 'research'\n",
      " 'attractivity' 'eeccp' 'activation' 'clairvoyant']\n",
      "['broadcast' 'and' 'ispreadrank' 'shows' 'amst' 'refereed'\n",
      " 'parallelizability' 'pmg' 'waga' 'with']\n",
      "['jffs' 'superlumt' 'only' 'hampering' 'kyoto' 'competed' 'therewith'\n",
      " 'teenage' 'new' 'think']\n",
      "['reusability' 'grover' 'ours' 'predesigned' 'subdialogues' 'pentium' 'by'\n",
      " 'objectives' 'forlinks' 'apertures']\n",
      "['anl' 'uar' 'build' 'andmemory' 'pnl' 'jaffe' 'synthesizes' 'convolving'\n",
      " 'make' 'schemeand']\n",
      "['knowledgebase' 'backtransactional' 'bets' 'nonbasic' 'with' 'ambienttalk'\n",
      " 'musicbrainz' 'privilege' \"verisoft's\" 'unigram']\n",
      "['verbal' 'translation' 'intersect' 'projected' 'harvesting' 'netscsi'\n",
      " 'karl' 'and' 'multilingual' 'prepcom']\n",
      "['octrees' 'gui' 'new' 'by' 'consumptionwe' 'with' 'gskj' 'flung'\n",
      " 'insightful' 'underpinned']\n",
      "['kademlia' 'believable' 'collected' \"artist's\" 'about' 'biannual'\n",
      " 'involving' 'new' 'poincare' 'the']\n",
      "['measuring' 'disposal' 'phaseload' 'research' 'constant' 'elgamal' 'pacc'\n",
      " 'hunting' 'downtown' 'pancyclic']\n",
      "['prehe' 'memorizable' 'greg' 'seek' 'csas' 'sign' 'frustration' 'mix'\n",
      " 'new' 'ose']\n",
      "['quel' 'by' 'and' 'pipeliner' 'varied' 'xcert' 'honoring' 'lfr'\n",
      " 'voluminos' 'sldnfa']\n",
      "['folds' 'research' 'less' 'donationbased' 'unranked' 'shows' 'sp' 'matlab'\n",
      " 'techniqueused' 'reinforced']\n",
      "['rbfn' 'legislative' 'radioactive' 'favorable' 'gerrymandering' 'only'\n",
      " 'aar' 'original' 'naming' 'about']\n",
      "['overconstrained' 'user' 'fuzziness' 'beneficial' 'bgc' 'commutativity'\n",
      " 'micropolygon' 'polytopes' 'solaria' 'brewka']\n",
      "['negated' 'about' 'hardwure' 'republishing' 'marriage' 'patch' 'smiles'\n",
      " 'cause' 'ssuite' 'encumbers']\n",
      "['fared' 'preoptimized' 'channeled' 'hla' 'ptf' 'ineffectual' 'estimate'\n",
      " 'sebek' 'teradata' 'predeployed']\n"
     ]
    }
   ],
   "source": [
    "for topic in topic_word_prob:\n",
    "    background = [\"about\",\"the\",\"and\",\"with\",\"all\",\"by\",\"only\",\"make\",\"build\",\"find\",\"shows\",\"less\",\"new\",\"research\"]\n",
    "    lst = vocab[np.argsort(topic)[::-1][:10]]\n",
    "    rand_bkgrd = random.choice(background,3,replace=False)\n",
    "    if (random.randint(1,10)>=2):\n",
    "        lst[random.randint(1,10)] = rand_bkgrd[1]\n",
    "        if (random.randint(1,10)>=5):\n",
    "            lst[random.randint(1,10)] = rand_bkgrd[2]\n",
    "            if (random.randint(1,10)>=5):\n",
    "                lst[random.randint(1,10)] = rand_bkgrd[0]\n",
    "    print lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) To get at more discriminative topics, we could do preprocessing to filter out the stopwords in our vocabulary before we even put it into the mixture model. However this technique would not eliminate background topics that are not stopwords but don't really convey additional meaning to the topic. To eliminate these non-discriminative words, we can look at the occurences of words across different topics. If a word exists in multiple topics and that those topics are unrelated, then that word is likely a background word. There are several ways to look at simmilarity between topics, including cosine simmilarity between vectors of word probabilities for each topic. These pre-processing and post-processing techniques should improve the discriminative power of the topics even if $\\lambda$ is set to an inappropriate value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PLSA_Mixture(SEED,DEBUG=False):\n",
    "    np.random.seed(SEED)\n",
    "    #Random Initialization then normalize probabilities\n",
    "    document_topic_prob = np.random.rand(ndoc, K+1 )  # P(z|pi) (K+1 topics for background)\n",
    "    #Normalize document topic probabilities\n",
    "    document_topic_prob = normalize(document_topic_prob)\n",
    "    topic_word_prob = np.random.rand(K+1, len(vocab))# P(w | theta)\n",
    "    topic_word_prob = normalize(topic_word_prob)\n",
    "    #too large to do random.rand(ndoc, len(vocab), K+1) , so simply initialize with all zeroes \n",
    "    topic_prob = np.random.rand(ndoc, len(vocab), K+1)# P(z | D,pi,theta)\n",
    "    topic_prob = normalize(topic_prob)\n",
    "\n",
    "    background_prob = np.random.rand(ndoc, len(vocab))# qy(y=1)=P(y=1| D,pi,theta)\n",
    "    background_prob = normalize(background_prob)\n",
    "\n",
    "    niter = 0\n",
    "    prev_loglikelihood =0\n",
    "    loglikelihood =999\n",
    "    loglikelihood_lst = []\n",
    "    while (abs(loglikelihood-prev_loglikelihood)>0.0001 or niter<=100): \n",
    "        if DEBUG: print \"Step #\",niter+1\n",
    "        # Compute log p(D|theta, pi) at the ith step \n",
    "        loglikelihood = 0\n",
    "        for d in np.arange(len(dlines)):\n",
    "            for w in np.arange(len(vocab)):\n",
    "                loglikelihood+= log(lmda*background_prob[d,w]+(1-lmda)*sum(document_topic_prob[d,:]*topic_word_prob[:, w]))\n",
    "        if DEBUG: print \"Log likelihood:\",str(loglikelihood)\n",
    "        if np.isnan(loglikelihood):\n",
    "            loglikelihood=99999\n",
    "        #matrix storing yij (if yij=0 then background; yij=1 then topic words)\n",
    "        Y = np.zeros([ndoc, len(vocab)])\n",
    "        if DEBUG: print \"Expectation Step \"\n",
    "        qzy=0\n",
    "        qy=0\n",
    "        for d, document in enumerate(dlines):\n",
    "            for w in np.arange(len(vocab)):\n",
    "                if Y[d][w]==1: \n",
    "                    # If Topic\n",
    "                    qzy = document_topic_prob[d,:]*topic_word_prob[:, w]\n",
    "                    #Finish computing qzy\n",
    "                    qzy = normalize(prob)\n",
    "                    topic_prob[d][w] = qzy\n",
    "                elif Y[d][w]==0: \n",
    "                    # If Background, Computing q_y \n",
    "                    numerator =0\n",
    "                    for k in np.arange(K):\n",
    "                        numerator += sum(document_topic_prob[d,:]*topic_word_prob[:,w])\n",
    "                    #Finish computing qy\n",
    "                    numerator = (1-lmda)*numerator\n",
    "                    p_wD = document.count(str(w))/len(document)\n",
    "                    denominator = lmda*p_wD + numerator\n",
    "                    qy = numerator/denominator\n",
    "                    background_prob[d][w] = qy\n",
    "    #             print \"qzy:\",qzy\n",
    "        if DEBUG: print \"Maximization step  \"\n",
    "\n",
    "        # update p(z=k|pi) for the n+1th step\n",
    "        for z in np.arange(K):\n",
    "            for w in np.arange(len(vocab)):    \n",
    "                ndk=0\n",
    "                for d in np.arange(len(dlines)):\n",
    "                    c = dlines[d].count(str(w)) #count of a word in the document \n",
    "                    ndk += c*topic_prob[d,w,z]*background_prob[d,w]\n",
    "                print ndk\n",
    "                topic_word_prob[z][w]=ndk\n",
    "            #Normalize across all documents for each topic \n",
    "            topic_word_prob[z] = normalize(topic_word_prob[z]) \n",
    "        # update p(w|theta_k) for the n+1th step\n",
    "        for d in np.arange(len(dlines)):\n",
    "            for z in np.arange(K):\n",
    "                nwk=0\n",
    "                for w in np.arange(len(vocab)):    \n",
    "                    c = dlines[d].count(str(w)) #count of a word in the document \n",
    "                    nwk += c*topic_prob[d,w,z]*background_prob[d,w]\n",
    "                document_topic_prob[d][z]=nwk\n",
    "            #Normalize across all words in the document\n",
    "            document_topic_prob[d] = normalize(document_topic_prob[d]) \n",
    "        #Update iteration and log-likelihood\n",
    "        niter +=1\n",
    "        loglikelihood_lst.append(loglikelihood)\n",
    "        prev_loglikelihood = loglikelihood\n",
    "    if DEBUG: print \"Finished\"\n",
    "    return loglikelihood_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loglikelihood_lst1 = PLSA_Mixture(787,DEBUG=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loglikelihood_lst2 = PLSA_Mixture(989,DEBUG=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loglikelihood_lst3 = PLSA_Mixture(236,DEBUG=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loglikelihood_lst4 = PLSA_Mixture(512,DEBUG=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1111cae10>"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEACAYAAACQx1DIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHvw/Ju4ULNnpW5UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111097b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loglikelihood_lst1,label=\"trial #1\")\n",
    "plt.plot(loglikelihood_lst2,label=\"trial #2\")\n",
    "plt.plot(loglikelihood_lst3,label=\"trial #3\")\n",
    "plt.plot(loglikelihood_lst4,label=\"trial #4\")\n",
    "plt.legend(loc = \"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) The four runs converge to a simmilar log likelihood value, even though they start off with different random seeds. We could see that the initial values of the loglikelihood in the first 20 steps are very different. But as the iterations progresses their likelihoods increase to simmilar values. In theory, this is not necessarily true in general, since the EM algorithm only guaruntees that we find a local optimum. So starting with a different seed could potentially mean that the EM algorithm could find a different local optimum if there are multiple local maxes for the likelihood value.\n",
    "\n",
    "###Therefore, if someone wants to achieve the highest possible log-likelihood for the model, I would suggest that they run the algorithm multiple times. Then best initial conditions (random seed) to use is the one that yields the highest possible log-likelihood for the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
